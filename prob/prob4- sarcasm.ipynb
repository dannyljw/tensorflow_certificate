{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f18be4ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-02T05:58:35.058497Z",
     "start_time": "2021-10-02T05:49:22.947741Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<OOV>  \t======>\t 1\n",
      "to  \t======>\t 2\n",
      "of  \t======>\t 3\n",
      "the  \t======>\t 4\n",
      "in  \t======>\t 5\n",
      "for  \t======>\t 6\n",
      "a  \t======>\t 7\n",
      "on  \t======>\t 8\n",
      "and  \t======>\t 9\n",
      "with  \t======>\t 10\n",
      "is  \t======>\t 11\n",
      "new  \t======>\t 12\n",
      "trump  \t======>\t 13\n",
      "man  \t======>\t 14\n",
      "from  \t======>\t 15\n",
      "at  \t======>\t 16\n",
      "about  \t======>\t 17\n",
      "you  \t======>\t 18\n",
      "by  \t======>\t 19\n",
      "this  \t======>\t 20\n",
      "after  \t======>\t 21\n",
      "be  \t======>\t 22\n",
      "up  \t======>\t 23\n",
      "out  \t======>\t 24\n",
      "that  \t======>\t 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-02 14:49:33.076363: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-10-02 14:49:33.107253: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fabfad4af20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-10-02 14:49:33.107316: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.4623 - acc: 0.7681\n",
      "Epoch 00001: val_loss improved from inf to 0.42647, saving model to my_checkpoint.ckpt\n",
      "625/625 [==============================] - 54s 86ms/step - loss: 0.4623 - acc: 0.7681 - val_loss: 0.4265 - val_acc: 0.7962\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.3634 - acc: 0.8332\n",
      "Epoch 00002: val_loss improved from 0.42647 to 0.39383, saving model to my_checkpoint.ckpt\n",
      "625/625 [==============================] - 55s 87ms/step - loss: 0.3634 - acc: 0.8332 - val_loss: 0.3938 - val_acc: 0.8188\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.3344 - acc: 0.8480\n",
      "Epoch 00003: val_loss improved from 0.39383 to 0.36755, saving model to my_checkpoint.ckpt\n",
      "625/625 [==============================] - 52s 82ms/step - loss: 0.3344 - acc: 0.8480 - val_loss: 0.3675 - val_acc: 0.8350\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.3126 - acc: 0.8597\n",
      "Epoch 00004: val_loss did not improve from 0.36755\n",
      "625/625 [==============================] - 54s 86ms/step - loss: 0.3126 - acc: 0.8597 - val_loss: 0.3744 - val_acc: 0.8320\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.2982 - acc: 0.8684\n",
      "Epoch 00005: val_loss did not improve from 0.36755\n",
      "625/625 [==============================] - 54s 86ms/step - loss: 0.2982 - acc: 0.8684 - val_loss: 0.4092 - val_acc: 0.8287\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.2858 - acc: 0.8754\n",
      "Epoch 00006: val_loss did not improve from 0.36755\n",
      "625/625 [==============================] - 50s 80ms/step - loss: 0.2858 - acc: 0.8754 - val_loss: 0.3810 - val_acc: 0.8272\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.2738 - acc: 0.8809\n",
      "Epoch 00007: val_loss did not improve from 0.36755\n",
      "625/625 [==============================] - 50s 80ms/step - loss: 0.2738 - acc: 0.8809 - val_loss: 0.4139 - val_acc: 0.8262\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.2662 - acc: 0.8875\n",
      "Epoch 00008: val_loss did not improve from 0.36755\n",
      "625/625 [==============================] - 55s 88ms/step - loss: 0.2662 - acc: 0.8875 - val_loss: 0.4323 - val_acc: 0.8246\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.2581 - acc: 0.8882\n",
      "Epoch 00009: val_loss did not improve from 0.36755\n",
      "625/625 [==============================] - 53s 85ms/step - loss: 0.2581 - acc: 0.8882 - val_loss: 0.4143 - val_acc: 0.8235\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.2507 - acc: 0.8936\n",
      "Epoch 00010: val_loss did not improve from 0.36755\n",
      "625/625 [==============================] - 54s 87ms/step - loss: 0.2507 - acc: 0.8936 - val_loss: 0.4062 - val_acc: 0.8278\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# There are 5 questions in this exam with increasing difficulty from 1-5.\n",
    "# Please note that the weight of the grade for the question is relative\n",
    "# to its difficulty. So your Category 1 question will score significantly\n",
    "# less than your Category 5 question.\n",
    "#\n",
    "# Don't use lambda layers in your model.\n",
    "# You do not need them to solve the question.\n",
    "# Lambda layers are not supported by the grading infrastructure.\n",
    "#\n",
    "# You must use the Submit and Test button to submit your model\n",
    "# at least once in this category before you finally submit your exam,\n",
    "# otherwise you will score zero for this category.\n",
    "# ======================================================================\n",
    "#\n",
    "# NLP QUESTION\n",
    "#\n",
    "# Build and train a classifier for the sarcasm dataset.\n",
    "# The classifier should have a final layer with 1 neuron activated by sigmoid as shown.\n",
    "# It will be tested against a number of sentences that the network hasn't previously seen\n",
    "# and you will be scored on whether sarcasm was correctly detected in those sentences.\n",
    "\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import urllib\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "def solution_model():\n",
    "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\n",
    "    urllib.request.urlretrieve(url, 'sarcasm.json')\n",
    "\n",
    "    # DO NOT CHANGE THIS CODE OR THE TESTS MAY NOT WORK\n",
    "    vocab_size = 1000\n",
    "    embedding_dim = 16\n",
    "    max_length = 120\n",
    "    trunc_type='post'\n",
    "    padding_type='post'\n",
    "    oov_tok = \"<OOV>\"\n",
    "    training_size = 20000\n",
    "    \n",
    "    sentences = []\n",
    "    labels = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    with open('sarcasm.json') as f:\n",
    "        datas = json.load(f)\n",
    "        datas[:5]\n",
    "    for data in datas:\n",
    "        sentences.append(data['headline'])\n",
    "        labels.append(data['is_sarcastic'])\n",
    "    \n",
    "    train_sentences = sentences[:training_size]\n",
    "    train_labels = labels[:training_size]\n",
    "    validation_sentences = sentences[training_size:]\n",
    "    validation_labels = labels[training_size:]\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(train_sentences)\n",
    "    for key, value in tokenizer.word_index.items():\n",
    "        print('{}  \\t======>\\t {}'.format(key, value))\n",
    "        if value == 25:\n",
    "            break\n",
    "\n",
    "    train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "    validation_sequences = tokenizer.texts_to_sequences(validation_sentences)\n",
    "\n",
    "    train_padded = pad_sequences(train_sequences, maxlen=max_length, truncating=trunc_type, padding=padding_type)\n",
    "    validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "\n",
    "    train_labels = np.array(train_labels)\n",
    "    validation_labels = np.array(validation_labels)\n",
    "\n",
    "    embedding_dim = 16\n",
    "\n",
    "    sample = np.array(train_padded[0])\n",
    "    sample\n",
    "    x = Embedding(vocab_size, embedding_dim, input_length=max_length)\n",
    "    x(sample)[0]\n",
    "\n",
    "    \n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "    # YOUR CODE HERE. KEEP THIS OUTPUT LAYER INTACT OR TESTS MAY FAIL\n",
    "            tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "            tf.keras.layers.Bidirectional(LSTM(64, return_sequences=True)),\n",
    "            tf.keras.layers.Bidirectional(LSTM(64)),\n",
    "            tf.keras.layers.Dense(32, activation='relu'),\n",
    "            tf.keras.layers.Dense(16, activation='relu'),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    checkpoint_path = 'my_checkpoint.ckpt'\n",
    "    checkpoint = ModelCheckpoint(checkpoint_path, \n",
    "                                 save_weights_only=True, \n",
    "                                 save_best_only=True, \n",
    "                                 monitor='val_loss',\n",
    "                                 verbose=1)\n",
    "\n",
    "    epochs=10\n",
    "    history = model.fit(train_padded, train_labels, \n",
    "                        validation_data=(validation_padded, validation_labels),\n",
    "                        callbacks=[checkpoint],\n",
    "                        epochs=epochs)\n",
    "\n",
    "    model.load_weights(checkpoint_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Note that you'll need to save your model as a .h5 like this.\n",
    "# When you press the Submit and Test button, your saved .h5 model will\n",
    "# be sent to the testing infrastructure for scoring\n",
    "# and the score will be returned to you.\n",
    "if __name__ == '__main__':\n",
    "    model = solution_model()\n",
    "    model.save(\"sarcasm.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c064c2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
